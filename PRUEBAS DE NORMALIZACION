#Reescalamiento minmax con python #práctica 1 p2
import numpy as np
from sklearn import preprocessing
feature = np.array([[-500.5], [-100.1], [0], [100.1],[900.9]])
minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))
scaled_feature = minmax_scale.fit_transform(feature)
scaled_feature

#Transformar datos para que tengan una media de 0 y una desv estandar de 1
#se utilizan las mismas librerías que en el ejemplo anterior
#practica 2 p2
x = np.array([[-1000.1],[-200.2], [500.5], [600.6],[9000.9]])
scaler = preprocessing.StandardScaler()
standardized = scaler.fit_transform(x)
standardized
#Se comprueba con los siguientes comandos 
print("Mean:", round(standardized.mean()))
print("Standard deviation:", standardized.std())

#alernativa cuando existen outliers
#practica 3 p2
import pandas as pd
data = pd.read_csv(r"C:\Users\Carmen\Downloads\AGO DIC 2022\ANALITICA Y VISUALIZACION\UNIDAD 2\mineros.csv") 
x=data
robust_scaler = preprocessing.RobustScaler()
robust_scaler.fit_transform(x)

#se busca reescalar los valores de las observaciones para tener norma unitaria 
#una longitud total de 1
#se utliza normalize con un argumento norm
#practica 4 p2
import numpy as np
from sklearn.preprocessing import Normalizer
features = np.array([[0.5, 0.5], [1.1, 3.4], [1.5, 20.2], [1.63, 34.4], [10.9, 3.3]])
normalizer = Normalizer(norm="l2")
normalizer.transform(features)
#este tipo de reescalamiento es útil cuando se tienen características equivalentes
#como por ejemplo clasificación de textos cuando cada palabra o grupo de n-palabras 
#es una caracteristica

#la norma euclideana es referida como L2 
#en argumento default x es una observación individual y xn es el valor de la 
#observación para la nth caracteristica 
features_l2_norm = Normalizer(norm="l2").transform(features)
features_l2_norm

#practica 5 p2 
#la norma Manhattan o L1 
features_l1_norm = Normalizer(norm="l1").transform(features)
features_l1_norm
#para ampliar las diferencias, por ejemplo, la norma 2 es la distancia en línea recta
#ebtre dos puntos y L1 puede ser vista como la distancia de una persona caminando en la calle 
#(una calle al norte, una calle al oeste) lo que también se llama 
#la norma Manhattan o la norma del taxi
#nótese que la norm='l1' reescala los valores de las observaciones para que sumen 1

print("Sum of the first observation\'s values:", features_l1_norm[0, 0] + features_l1_norm[0, 1])



#MANEJO DE OUTLIERS
#se busca identificar observaciones extremas 
#la primera estrategia para manejarlos es simplemente quitarlos

import pandas as pd
houses = pd.DataFrame()
houses['Price'] = [534433, 392333, 293222, 4322032]
houses['Bathrooms'] = [2, 3.5, 2, 116]
houses['Square_Feet'] = [1500, 2500, 1500, 48000]
houses[houses['Bathrooms'] < 20]

#segundo caso: podemos marcarlos 
import numpy as np
houses["Outlier"] = np.where(houses["Bathrooms"] < 20, 0, 1)
houses

#tercer caso
#podemos transformar la característica para amortiguar el efecto del valor atípico 
houses["Log_Of_Square_Feet"] = [np.log(x) for x in houses["Square_Feet"]]
houses

#REDUCCIÓN DE DIMENSIONALIDAD 
#NO ES OBVIO CUANDO TENEMOS ÚNICAMENTE 2 DIMENSIONES
#PERO SE VUELVE ÚTIL CUANDO TENEMOS DATOS CON MÁS DE 2 DIMENSIONES
#LOS DATOS CONLOS QUE SE VA A TRABAJAR SON IMÁGENES DE 8X8 PIXELES
#POR LO TANTO LOS DATOS TIENEN 64 DIMENSIONES 
#SE UTUILIZA EL ANÁLISIS DE COMPONENTES PRINCIPALES (PCA)
#PARA TENER UN NUMERO DE DIMENSIONES MANEJABLE

from sklearn.datasets import load_digits
digits = load_digits()
digits.data.shape

pca = PCA(2) # project from 64 to 2 dimensions
projected = pca.fit_transform(digits.data)
print(digits.data.shape)
print(projected.shape)  

import matplotlib as plt
pyplot.scatter(projected[:, 0], projected[:, 1], c=digits.target, edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('spectral', 10))
plt.xlabel('component 1')
plt.ylabel('component 2')
plt.colorbar();



